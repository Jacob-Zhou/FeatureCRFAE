From c21de9fd3dff9168b16e828bfc2cf8a55f54ec5e Mon Sep 17 00:00:00 2001
From: Jacob-Zhou <jacob_zhou@outlook.com>
Date: Mon, 7 Mar 2022 12:38:39 +0800
Subject: [PATCH] modify for crfae

---
 elmoformanylangs/elmo.py | 115 +++++++++++++++++++++++------------------------
 1 file changed, 55 insertions(+), 60 deletions(-)

diff --git a/elmoformanylangs/elmo.py b/elmoformanylangs/elmo.py
index aeb314c..ade3ea2 100644
--- a/elmoformanylangs/elmo.py
+++ b/elmoformanylangs/elmo.py
@@ -13,8 +13,6 @@ from .frontend import create_one_batch
 from .frontend import Model
 import numpy as np
 
-logger = logging.getLogger('elmoformanylangs')
-
 
 def read_list(sents, max_chars=None):
     """
@@ -51,7 +49,15 @@ def recover(li, ind):
 
 
 # shuffle training examples and create mini-batches
-def create_batches(x, batch_size, word2id, char2id, config, perm=None, shuffle=False, sort=True, text=None):
+def create_batches(x,
+                   batch_size,
+                   word2id,
+                   char2id,
+                   config,
+                   perm=None,
+                   shuffle=False,
+                   sort=True,
+                   text=None):
     ind = list(range(len(x)))
     lst = perm or list(range(len(x)))
     if shuffle:
@@ -71,15 +77,19 @@ def create_batches(x, batch_size, word2id, char2id, config, perm=None, shuffle=F
     nbatch = (len(x) - 1) // size + 1
     for i in range(nbatch):
         start_id, end_id = i * size, (i + 1) * size
-        bw, bc, blens, bmasks = create_one_batch(x[start_id: end_id], word2id, char2id, config, sort=sort)
+        bw, bc, blens, bmasks = create_one_batch(x[start_id:end_id],
+                                                 word2id,
+                                                 char2id,
+                                                 config,
+                                                 sort=sort)
         sum_len += sum(blens)
         batches_w.append(bw)
         batches_c.append(bc)
         batches_lens.append(blens)
         batches_masks.append(bmasks)
-        batches_ind.append(ind[start_id: end_id])
+        batches_ind.append(ind[start_id:end_id])
         if text is not None:
-            batches_text.append(text[start_id: end_id])
+            batches_text.append(text[start_id:end_id])
 
     if sort:
         perm = list(range(nbatch))
@@ -92,8 +102,6 @@ def create_batches(x, batch_size, word2id, char2id, config, perm=None, shuffle=F
         if text is not None:
             batches_text = [batches_text[i] for i in perm]
 
-    logger.info("{} batches, avg len: {:.1f}".format(
-        nbatch, sum_len / len(x)))
     recover_ind = [item for sublist in batches_ind for item in sublist]
     if text is not None:
         return batches_w, batches_c, batches_lens, batches_masks, batches_text, recover_ind
@@ -110,8 +118,11 @@ class Embedder(object):
         # torch.cuda.set_device(1)
         self.use_cuda = torch.cuda.is_available()
         # load the model configurations
-        args2 = dict2namedtuple(json.load(codecs.open(
-            os.path.join(self.model_dir, 'config.json'), 'r', encoding='utf-8')))
+        args2 = dict2namedtuple(
+            json.load(
+                codecs.open(os.path.join(self.model_dir, 'config.json'),
+                            'r',
+                            encoding='utf-8')))
 
         config_path = os.path.join(self.model_dir, args2.config_path)
         # Some of the available models may have the config in the
@@ -120,18 +131,19 @@ class Embedder(object):
         if not os.path.exists(config_path):
             config_path = os.path.join(self.model_dir,
                                        os.path.split(config_path)[1])
-            logger.warning("Could not find config.  Trying " + config_path)
         # In many cases, such as the publicly available English model,
         # the config is one of the default provided configs in
         # elmoformanylangs/configs
         if not os.path.exists(config_path):
-            config_path = os.path.join(os.path.split(__file__)[0], "configs",
-                                       os.path.split(config_path)[1])
-            logger.warning("Could not find config.  Trying " + config_path)
+            config_path = os.path.join(
+                os.path.split(__file__)[0], "configs",
+                os.path.split(config_path)[1])
 
         if not os.path.exists(config_path):
-            raise FileNotFoundError("Could not find the model config in either the model directory "
-                                    "or the default configs.  Path in config file: %s" % args2.config_path)
+            raise FileNotFoundError(
+                "Could not find the model config in either the model directory "
+                "or the default configs.  Path in config file: %s" %
+                args2.config_path)
 
         with open(config_path, 'r') as fin:
             config = json.load(fin)
@@ -139,7 +151,9 @@ class Embedder(object):
         # For the model trained with character-based word encoder.
         if config['token_embedder']['char_dim'] > 0:
             self.char_lexicon = {}
-            with codecs.open(os.path.join(self.model_dir, 'char.dic'), 'r', encoding='utf-8') as fpi:
+            with codecs.open(os.path.join(self.model_dir, 'char.dic'),
+                             'r',
+                             encoding='utf-8') as fpi:
                 for line in fpi:
                     tokens = line.strip().split('\t')
                     if len(tokens) == 1:
@@ -147,9 +161,10 @@ class Embedder(object):
                     token, i = tokens
                     self.char_lexicon[token] = int(i)
             char_emb_layer = EmbeddingLayer(
-                config['token_embedder']['char_dim'], self.char_lexicon, fix_emb=False, embs=None)
-            logger.info('char embedding size: ' +
-                        str(len(char_emb_layer.word2id)))
+                config['token_embedder']['char_dim'],
+                self.char_lexicon,
+                fix_emb=False,
+                embs=None)
         else:
             self.char_lexicon = None
             char_emb_layer = None
@@ -157,7 +172,9 @@ class Embedder(object):
         # For the model trained with word form word encoder.
         if config['token_embedder']['word_dim'] > 0:
             self.word_lexicon = {}
-            with codecs.open(os.path.join(self.model_dir, 'word.dic'), 'r', encoding='utf-8') as fpi:
+            with codecs.open(os.path.join(self.model_dir, 'word.dic'),
+                             'r',
+                             encoding='utf-8') as fpi:
                 for line in fpi:
                     tokens = line.strip().split('\t')
                     if len(tokens) == 1:
@@ -165,9 +182,10 @@ class Embedder(object):
                     token, i = tokens
                     self.word_lexicon[token] = int(i)
             word_emb_layer = EmbeddingLayer(
-                config['token_embedder']['word_dim'], self.word_lexicon, fix_emb=False, embs=None)
-            logger.info('word embedding size: ' +
-                        str(len(word_emb_layer.word2id)))
+                config['token_embedder']['word_dim'],
+                self.word_lexicon,
+                fix_emb=False,
+                embs=None)
         else:
             self.word_lexicon = None
             word_emb_layer = None
@@ -178,7 +196,6 @@ class Embedder(object):
         if self.use_cuda:
             model.cuda()
 
-        logger.info(str(model))
         model.load_model(self.model_dir)
 
         # read test data according to input format
@@ -187,47 +204,25 @@ class Embedder(object):
         model.eval()
         return model, config
 
-    def sents2elmo(self, sents, output_layer=-1):
-        read_function = read_list
-
-        if self.config['token_embedder']['name'].lower() == 'cnn':
-            test, text = read_function(sents, self.config['token_embedder']['max_characters_per_token'])
-        else:
-            test, text = read_function(sents)
+    def sents2elmo(self, sents):
+        test, text = read_list(
+            sents, self.config['token_embedder']['max_characters_per_token'])
 
         # create test batches from the input data.
         test_w, test_c, test_lens, test_masks, test_text, recover_ind = create_batches(
-            test, self.batch_size, self.word_lexicon, self.char_lexicon, self.config, text=text)
-
-        cnt = 0
+            test,
+            self.batch_size,
+            self.word_lexicon,
+            self.char_lexicon,
+            self.config,
+            text=text)
 
         after_elmo = []
-        for w, c, lens, masks, texts in zip(test_w, test_c, test_lens, test_masks, test_text):
+        for w, c, lens, masks, texts in zip(test_w, test_c, test_lens,
+                                            test_masks, test_text):
             output = self.model.forward(w, c, masks)
             for i, text in enumerate(texts):
-
-                if self.config['encoder']['name'].lower() == 'lstm':
-                    data = output[i, 1:lens[i]-1, :].data
-                    if self.use_cuda:
-                        data = data.cpu()
-                    data = data.numpy()
-                elif self.config['encoder']['name'].lower() == 'elmo':
-                    data = output[:, i, 1:lens[i]-1, :].data
-                    if self.use_cuda:
-                        data = data.cpu()
-                    data = data.numpy()
-
-                if output_layer == -1:
-                    payload = np.average(data, axis=0)
-                elif output_layer == -2:
-                    payload = data
-                else:
-                    payload = data[output_layer]
-                after_elmo.append(payload)
-
-                cnt += 1
-                if cnt % 1000 == 0:
-                    logger.info('Finished {0} sentences.'.format(cnt))
-
+                data = output[:, i, 1:lens[i] - 1, :].data
+                after_elmo.append(data)
         after_elmo = recover(after_elmo, recover_ind)
         return after_elmo
-- 
1.8.3.1

